{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65921a1a-75e4-435b-9417-0c1459db2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from longformer.longformer import Longformer, LongformerConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a88fb98-a9e1-4490-8c2c-6bbea25090b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = LongformerConfig(attention_window=[512]* 12, attention_dilation=[1] * 12, autoregressive=False, attention_mode='sliding_chunks')\n",
    "cfg.max_position_embeddings = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfca958-5e9f-4f2c-8ef4-4d708a1643e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Longformer(cfg, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7426f1cd-90db-4f39-a02f-904d854c5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from typing import Optional, Union, Tuple\n",
    "from transformers.models.longformer.modeling_longformer import LongformerPreTrainedModel, LongformerSequenceClassifierOutput\n",
    "\n",
    "class LongformerForSequenceClassification(LongformerPreTrainedModel):\n",
    "    def __init__(self, config, num_labels, id2label, label2id, model: Longformer):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        self.longformer = model\n",
    "        self.classifier = LongformerClassificationHead(config)\n",
    "\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.Tensor] = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        global_attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        token_type_ids: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.Tensor] = None,\n",
    "        inputs_embeds: Optional[torch.Tensor] = None,\n",
    "        labels: Optional[torch.Tensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, LongformerSequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if global_attention_mask is None:\n",
    "            logger.warning_once(\"Initializing global attention on CLS token...\")\n",
    "            global_attention_mask = torch.zeros_like(input_ids)\n",
    "            # global attention on cls token\n",
    "            global_attention_mask[:, 0] = 1\n",
    "\n",
    "        outputs = self.longformer(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            # global_attention_mask=global_attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "\n",
    "    #     self_outputs = self.self(\n",
    "    # 334         hidden_states,\n",
    "    # 335         attention_mask,\n",
    "    # 336         head_mask,\n",
    "    # 337         encoder_hidden_states,\n",
    "    # 338         encoder_attention_mask,\n",
    "    # 339         past_key_value,\n",
    "    # 340         output_attentions,\n",
    "    # 341     )\n",
    "        \n",
    "        sequence_output = outputs[0]\n",
    "        logits = self.classifier(sequence_output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            labels = labels.to(logits.device)\n",
    "\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return LongformerSequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "            global_attentions=outputs.global_attentions,\n",
    "        )\n",
    "\n",
    "\n",
    "class LongformerClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, hidden_states, **kwargs):\n",
    "        hidden_states = hidden_states[:, 0, :]  # take <s> token (equiv. to [CLS])\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        hidden_states = self.dense(hidden_states)\n",
    "        hidden_states = torch.tanh(hidden_states)\n",
    "        hidden_states = self.dropout(hidden_states)\n",
    "        output = self.out_proj(hidden_states)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b76e8-ea20-4fa7-9e77-9eb364f818ed",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaae3e1-59c8-4d7c-b323-f8a91dbc1ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c60740-deb4-4667-ab3a-31378a52bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"config.json\") as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9544f5-af4c-40e2-a6d4-629bc18dc8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a6bbba-8a64-4814-a85d-fe8757b83fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "model_name = config[\"model_name\"]\n",
    "model_path = (\n",
    "    model_name.split(\"/\")[-1].replace(\"-\", \"_\") + \"_text_classification_imdb\" + \"_adamw\"\n",
    ")\n",
    "BATCH_SIZE = config[\"batch_size\"]\n",
    "NUM_EPOCHS = config[\"num_epochs\"]\n",
    "\n",
    "if os.path.exists('model_path'):\n",
    "    shutil.rmtree('model_path')\n",
    "\n",
    "imdb = load_dataset(\"imdb\", cache_dir=\"model_path\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding='max_length', max_length=4096)\n",
    "\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True, remove_columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a83cb18-32c7-4194-917d-07d498b22597",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)\n",
    "\n",
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad782c8-6b19-4627-b0f7-a795bf621a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "longformer = Longformer(cfg, add_pooling_layer=False)\n",
    "model = LongformerForSequenceClassification(\n",
    "    cfg, num_labels=2, id2label=id2label, label2id=label2id, model=longformer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549e1536-df15-425a-bc99-e609739fff4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on this\n",
    "\n",
    "# https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue_no_trainer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c763f9-8c89-4cd7-8717-7c694aa4032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(tokenized_imdb['train'].with_format('torch'), shuffle=True, collate_fn=data_collator, batch_size=32)\n",
    "eval_dataloader = DataLoader(tokenized_imdb['test'].with_format('torch'), collate_fn=data_collator, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c090f6-32b6-479f-a723-f06f17f5ed32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e75e456-46c6-4688-acd3-b411b6f4c7c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001 # this is default one from adamW\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f284c-25f4-4357-97b1-f63f9c4bf2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from transformers import get_scheduler\n",
    "from accelerate import Accelerator\n",
    "\n",
    "\n",
    "max_train_steps = None # left it as None, to be calculated dynamically\n",
    "\n",
    "num_train_epochs = 3\n",
    "gradient_accumulation_steps = 1\n",
    "\n",
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "if max_train_steps is None:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler_type = 'linear' \n",
    "# possible values \n",
    "# class SchedulerType(ExplicitEnum):\n",
    "#     LINEAR = \"linear\"\n",
    "#     COSINE = \"cosine\"\n",
    "#     COSINE_WITH_RESTARTS = \"cosine_with_restarts\"\n",
    "#     POLYNOMIAL = \"polynomial\"\n",
    "#     CONSTANT = \"constant\"\n",
    "#     CONSTANT_WITH_WARMUP = \"constant_with_warmup\"\n",
    "#     INVERSE_SQRT = \"inverse_sqrt\"\n",
    "#     REDUCE_ON_PLATEAU = \"reduce_lr_on_plateau\"\n",
    "#     COSINE_WITH_MIN_LR = \"cosine_with_min_lr\"\n",
    "#     WARMUP_STABLE_DECAY = \"warmup_stable_decay\"\n",
    "\n",
    "\n",
    "num_warmup_steps = 0 # was default\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=lr_scheduler_type,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=num_warmup_steps,\n",
    "    num_training_steps=max_train_steps,\n",
    ")\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "# Prepare everything with our `accelerator`.\n",
    "model, optimizer, train_dataloader, eval_dataloader, lr_scheduler = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader, lr_scheduler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aac0271-23c4-47e1-ac7d-de1e48d3478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)\n",
    "\n",
    "if overrode_max_train_steps:\n",
    "    max_train_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "# Afterwards we recalculate our number of training epochs\n",
    "num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "# Figure out how many steps we should save the Accelerator states\n",
    "\n",
    "\n",
    "checkpointing_steps = 'epoch' # (can be integer values or idk, check in the script - link above) \n",
    "if checkpointing_steps is not None and checkpointing_steps.isdigit():\n",
    "    checkpointing_steps = int(checkpointing_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13b805b-a0de-4396-a469-4b3301d9925e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train!\n",
    "total_batch_size = 32 * accelerator.num_processes * gradient_accumulation_steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064975c7-19ee-4d4f-adb5-bf99fec55cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "from accelerate.logging import get_logger\n",
    "\n",
    "logger = get_logger('logger')\n",
    "\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "logger.info(accelerator.state, main_process_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f32b6a2-6677-47be-9aa9-15e39676cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "completed_steps = 0\n",
    "starting_epoch = 0\n",
    "\n",
    "# Potentially load in the weights and states from a previous save\n",
    "# if args.resume_from_checkpoint:\n",
    "#     if args.resume_from_checkpoint is not None or args.resume_from_checkpoint != \"\":\n",
    "#         checkpoint_path = args.resume_from_checkpoint\n",
    "#         path = os.path.basename(args.resume_from_checkpoint)\n",
    "#     else:\n",
    "#         # Get the most recent checkpoint\n",
    "#         dirs = [f.name for f in os.scandir(os.getcwd()) if f.is_dir()]\n",
    "#         dirs.sort(key=os.path.getctime)\n",
    "#         path = dirs[-1]  # Sorts folders by date modified, most recent checkpoint is the last\n",
    "#         checkpoint_path = path\n",
    "#         path = os.path.basename(checkpoint_path)\n",
    "\n",
    "#     accelerator.print(f\"Resumed from checkpoint: {checkpoint_path}\")\n",
    "#     accelerator.load_state(checkpoint_path)\n",
    "#     # Extract `epoch_{i}` or `step_{i}`\n",
    "#     training_difference = os.path.splitext(path)[0]\n",
    "\n",
    "#     if \"epoch\" in training_difference:\n",
    "#         starting_epoch = int(training_difference.replace(\"epoch_\", \"\")) + 1\n",
    "#         resume_step = None\n",
    "#         completed_steps = starting_epoch * num_update_steps_per_epoch\n",
    "#     else:\n",
    "#         # need to multiply `gradient_accumulation_steps` to reflect real steps\n",
    "#         resume_step = int(training_difference.replace(\"step_\", \"\")) * args.gradient_accumulation_steps\n",
    "#         starting_epoch = resume_step // len(train_dataloader)\n",
    "#         completed_steps = resume_step // args.gradient_accumulation_steps\n",
    "#         resume_step -= starting_epoch * len(train_dataloader)\n",
    "\n",
    "# update the progress_bar if load from checkpoint\n",
    "# progress_bar.update(completed_steps)\n",
    "\n",
    "output_dir = model_path\n",
    "\n",
    "for epoch in range(starting_epoch, num_train_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    active_dataloader = train_dataloader\n",
    "    \n",
    "    for step, batch in enumerate(active_dataloader):\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # We keep track of the loss at each epoch\n",
    "        total_loss += loss.detach().float()\n",
    "        loss = loss / gradient_accumulation_steps\n",
    "        accelerator.backward(loss)\n",
    "        if step % gradient_accumulation_steps == 0 or step == len(train_dataloader) - 1:\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "            completed_steps += 1\n",
    "\n",
    "        if isinstance(checkpointing_steps, int):\n",
    "            if completed_steps % checkpointing_steps == 0:\n",
    "                output_dir = f\"step_{completed_steps}\"\n",
    "                if output_dir is not None:\n",
    "                    output_dir = os.path.join(output_dir, output_dir)\n",
    "                accelerator.save_state(output_dir)\n",
    "\n",
    "        if completed_steps >= max_train_steps:\n",
    "            break\n",
    "\n",
    "    model.eval()\n",
    "    samples_seen = 0\n",
    "    for step, batch in enumerate(eval_dataloader):\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        predictions = outputs.logits.argmax(dim=-1) if not is_regression else outputs.logits.squeeze()\n",
    "        predictions, references = accelerator.gather((predictions, batch[\"labels\"]))\n",
    "        # If we are in a multiprocess environment, the last batch has duplicates\n",
    "        if accelerator.num_processes > 1:\n",
    "            if step == len(eval_dataloader) - 1:\n",
    "                predictions = predictions[: len(eval_dataloader.dataset) - samples_seen]\n",
    "                references = references[: len(eval_dataloader.dataset) - samples_seen]\n",
    "            else:\n",
    "                samples_seen += references.shape[0]\n",
    "        metric.add_batch(\n",
    "            predictions=predictions,\n",
    "            references=references,\n",
    "        )\n",
    "\n",
    "    eval_metric = metric.compute()\n",
    "    logger.info(f\"epoch {epoch}: {eval_metric}\")\n",
    "\n",
    "    accelerator.log(\n",
    "        {\n",
    "            \"accuracy\" if args.task_name is not None else \"glue\": eval_metric,\n",
    "            \"train_loss\": total_loss.item() / len(train_dataloader),\n",
    "            \"epoch\": epoch,\n",
    "            \"step\": completed_steps,\n",
    "        },\n",
    "        step=completed_steps,\n",
    "    )\n",
    "\n",
    "    # if args.push_to_hub and epoch < args.num_train_epochs - 1:\n",
    "    #     accelerator.wait_for_everyone()\n",
    "    #     unwrapped_model = accelerator.unwrap_model(model)\n",
    "    #     unwrapped_model.save_pretrained(\n",
    "    #         args.output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    #     )\n",
    "    #     if accelerator.is_main_process:\n",
    "    #         tokenizer.save_pretrained(args.output_dir)\n",
    "    #         api.upload_folder(\n",
    "    #             commit_message=f\"Training in progress epoch {epoch}\",\n",
    "    #             folder_path=args.output_dir,\n",
    "    #             repo_id=repo_id,\n",
    "    #             repo_type=\"model\",\n",
    "    #             token=args.hub_token,\n",
    "    #         )\n",
    "\n",
    "    if checkpointing_steps == \"epoch\":\n",
    "        output_dir_epoch = f\"epoch_{epoch}\"\n",
    "        if output_dir is not None:\n",
    "            output_dir_epoch = os.path.join(output_dir, output_dir_epoch)\n",
    "        accelerator.save_state(output_dir_epoch)\n",
    "\n",
    "\n",
    "accelerator.end_training()\n",
    "\n",
    "if output_dir is not None:\n",
    "    accelerator.wait_for_everyone()\n",
    "    unwrapped_model = accelerator.unwrap_model(model)\n",
    "    unwrapped_model.save_pretrained(\n",
    "        output_dir, is_main_process=accelerator.is_main_process, save_function=accelerator.save\n",
    "    )\n",
    "    if accelerator.is_main_process:\n",
    "        tokenizer.save_pretrained(output_dir)\n",
    "        # if args.push_to_hub:\n",
    "        #     api.upload_folder(\n",
    "        #         commit_message=\"End of training\",\n",
    "        #         folder_path=args.output_dir,\n",
    "        #         repo_id=repo_id,\n",
    "        #         repo_type=\"model\",\n",
    "        #         token=args.hub_token,\n",
    "        #     )\n",
    "\n",
    "if output_dir is not None:\n",
    "    all_results = {f\"eval_{k}\": v for k, v in eval_metric.items()}\n",
    "    with open(os.path.join(output_dir, \"all_results.json\"), \"w\") as f:\n",
    "        json.dump(all_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65dde9e6-ae1e-4d25-8404-85b33343b33e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
